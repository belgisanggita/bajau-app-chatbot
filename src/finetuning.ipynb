{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9Yd8DrPafs"
      },
      "source": [
        "# **INSTALL LIBRARY YANG DIBUTUHKAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6s6AcPt08K5",
        "outputId": "6f09c487-3548-458d-b276-501a4f88f828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.7.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m761.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.34.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.14)\n",
            "Collecting bitsandBytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandBytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandBytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandBytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandBytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandBytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandBytes\n",
            "Successfully installed bitsandBytes-0.46.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=a98930d927c0b77f390ebe23fd92290504eab08e39cde7d89cf6ac66f99aa642\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "Collecting groq\n",
            "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.31.0\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=079e7d1f65dd0fcf4eb48e40a1cc8ca4c181c360c24fa0902c7e3c2833615458\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install bitsandBytes\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install GPUtil\n",
        "!pip install groq\n",
        "!pip install rouge_score\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4eq2k1vRtr8"
      },
      "source": [
        "# **DATASET PREPARATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_DcLk8SWvj3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from groq import Groq\n",
        "\n",
        "# 1. Load API key from environment (or use default for development)\n",
        "API_KEY = os.getenv(\"GROQ_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "client = Groq(api_key=API_KEY)\n",
        "MODEL = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "# 2. Seed examples (few-shot)\n",
        "seed_examples = [\n",
        "    {\n",
        "        \"input\": \"How do I reset my password in the Bajau E-Commerce mobile app?\",\n",
        "        \"output\": (\n",
        "            \"1. Open the Bajau app\\n\"\n",
        "            \"2. Tap “Forgot Password” on the login screen\\n\"\n",
        "            \"3. Enter your email\\n\"\n",
        "            \"4. Tap “Submit” to get reset link\\n\"\n",
        "            \"5. Check your inbox and click the link\\n\"\n",
        "            \"6. Enter & confirm new password\\n\"\n",
        "            \"7. Tap “Save”\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How do I create a new account in the Bajau E-Commerce mobile app?\",\n",
        "        \"output\": (\n",
        "            \"1. Open the Bajau app\\n\"\n",
        "            \"2. Tap “Sign Up”\\n\"\n",
        "            \"3. Enter name, email, password\\n\"\n",
        "            \"4. Agree Terms of Service\\n\"\n",
        "            \"5. Tap “Create Account”\\n\"\n",
        "            \"6. Verify email via link\\n\"\n",
        "            \"7. Log in with new credentials\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How do I use the chatbot in the Bajau E-Commerce mobile app?\",\n",
        "        \"output\": (\n",
        "            \"1. Open the Bajau app\\n\"\n",
        "            \"2. Tap chat icon\\n\"\n",
        "            \"3. Type your question\\n\"\n",
        "            \"4. Press “Send”\\n\"\n",
        "            \"5. Read bot’s reply\\n\"\n",
        "            \"6. Provide any extra details\\n\"\n",
        "            \"7. Tap “Escalate” to talk with agent\"\n",
        "        )\n",
        "    },\n",
        "    # Tambahkan contoh tambahan jika diinginkan\n",
        "]\n",
        "\n",
        "# 3. Build the conversation messages for few-shot\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are an assistant that ONLY replies with a JSON array of objects, \"\n",
        "            \"each having `input` (a user question) and `output` (numbered steps). \"\n",
        "            \"No extra text, no markdown fences.\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "for ex in seed_examples:\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps([ex], ensure_ascii=False)\n",
        "    })\n",
        "    messages.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps([ex], ensure_ascii=False)\n",
        "    })\n",
        "\n",
        "# 4. Final user prompt to generate 100 new examples\n",
        "messages.append({\n",
        "    \"role\": \"user\",\n",
        "    \"content\": (\n",
        "        \"PT Bajau Escorindo is one of the forefront of IT innovation and Solutions company in Indonesia.\"\n",
        "        \"Bajau just release it's own E-Commerce application named Bajau E-Commerce.\"\n",
        "        \"Now generate 150 new examples following exactly that JSON-array format. \"\n",
        "        \"OUTPUT MUST BE A PURE JSON ARRAY—no extra text or fences.\"\n",
        "    )\n",
        "})\n",
        "\n",
        "# 5. Call the API\n",
        "resp = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=messages,\n",
        "    max_tokens=15000,\n",
        "    temperature=0.7,\n",
        "    n=1,\n",
        "    stop=[\"]\"],  # stop at end of JSON array\n",
        ")\n",
        "\n",
        "raw = resp.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smpFFajlRxuv",
        "outputId": "824d9f55-d7bd-4b33-e7c4-d5c24095f324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success: 186 data written to 'dataset.json'.\n"
          ]
        }
      ],
      "source": [
        "# 6. Clean up the raw text to extract the JSON array\n",
        "clean = raw.strip()\n",
        "if clean.startswith(\"```\"):\n",
        "    clean = clean.split(\"```\", 2)[1].rsplit(\"```\", 1)[0]\n",
        "match = re.search(r'(\\[.*)$', clean, flags=re.DOTALL)\n",
        "if not match:\n",
        "    raise ValueError(\"Cannot find JSON array in model output.\")\n",
        "clean = match.group(1)\n",
        "if not clean.rstrip().endswith(\"]\"):\n",
        "    clean = clean.rstrip() + \"]\"\n",
        "\n",
        "# 7. Repair by extracting well-formed JSON objects\n",
        "object_pattern = r'\\{(?:[^{}]|\"(?:\\\\.|[^\"\\\\])*\")*\\}'\n",
        "objects = re.findall(object_pattern, clean, flags=re.DOTALL)\n",
        "repaired_json = \"[\" + \",\".join(objects) + \"]\"\n",
        "\n",
        "# 8. Parse the repaired JSON\n",
        "try:\n",
        "    data = json.loads(repaired_json)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(\"Failed to parse JSON after repair:\", e)\n",
        "    print(repaired_json[:500])\n",
        "    raise\n",
        "\n",
        "# 9. Save to file\n",
        "output_file = \"dataset.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Success: {len(data)} data written to '{output_file}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZBwMuyW9uis",
        "outputId": "5c653880-76c3-4b72-fcb1-d041d65ead0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input': 'How do I track my order in the Bajau E-Commerce mobile app?', 'output': '1. Open the Bajau app\\n2. Tap “My Account”\\n3. Select “My Orders”\\n4. Choose the order to track\\n5. Tap “Track Order”\\n6. View order status\\n7. Contact support if needed'}\n"
          ]
        }
      ],
      "source": [
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imD8gxy3Yv-z",
        "outputId": "9c55183e-bf28-4a81-dc04-5b7366f6b082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- First 5 Examples from dataset.json ---\n",
            "Example 1:\n",
            "{\n",
            "  \"input\": \"How do I track my order in the Bajau E-Commerce mobile app?\",\n",
            "  \"output\": \"1. Open the Bajau app\\n2. Tap “My Account”\\n3. Select “My Orders”\\n4. Choose the order to track\\n5. Tap “Track Order”\\n6. View order status\\n7. Contact support if needed\"\n",
            "}\n",
            "--------------------\n",
            "Example 2:\n",
            "{\n",
            "  \"input\": \"What payment methods are available in the Bajau E-Commerce mobile app?\",\n",
            "  \"output\": \"1. Open the Bajau app\\n2. Tap “My Account”\\n3. Select “Payment Methods”\\n4. View available payment options\\n5. Add new payment method\\n6. Save changes\\n7. Proceed to checkout\"\n",
            "}\n",
            "--------------------\n",
            "Example 3:\n",
            "{\n",
            "  \"input\": \"How do I return a product in the Bajau E-Commerce mobile app?\",\n",
            "  \"output\": \"1. Open the Bajau app\\n2. Tap “My Account”\\n3. Select “My Orders”\\n4. Choose the order to return\\n5. Tap “Return Item”\\n6. Follow return instructions\\n7. Wait for refund or replacement\"\n",
            "}\n",
            "--------------------\n",
            "Example 4:\n",
            "{\n",
            "  \"input\": \"Can I change my order after it's been placed in the Bajau E-Commerce mobile app?\",\n",
            "  \"output\": \"1. Open the Bajau app\\n2. Tap “My Account”\\n3. Select “My Orders”\\n4. Choose the order to modify\\n5. Tap “Edit Order”\\n6. Make changes to order\\n7. Confirm changes\"\n",
            "}\n",
            "--------------------\n",
            "Example 5:\n",
            "{\n",
            "  \"input\": \"How do I use a promo code in the Bajau E-Commerce mobile app?\",\n",
            "  \"output\": \"1. Open the Bajau app\\n2. Tap “My Account”\\n3. Select “My Coupons”\\n4. Enter promo code\\n5. Tap “Apply”\\n6. View discounted price\\n7. Proceed to checkout\"\n",
            "}\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load the data from dataset.json\n",
        "try:\n",
        "    with open(\"dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    # Print the first 5 examples\n",
        "    print(\"--- First 5 Examples from dataset.json ---\")\n",
        "    for i, example in enumerate(dataset[:5]):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(json.dumps(example, indent=2, ensure_ascii=False))\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: dataset.json not found. Please ensure the previous cell ran successfully to create the file.\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON from dataset.json: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvuPiEI-Ww5J"
      },
      "source": [
        "# **DATASET PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "weTc6BOu-APD",
        "outputId": "6a5f8a5e-b404-4f6b-f29d-eef1dcbe5fa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86b4099d42a14e41833e7e53f269ade8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2ff7dae03254fd7876fc53fc3b75918",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c3facb3f23f41798fb972280e2e2e03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69525b04b20541eba2d63b7f7ec69fed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76a8bc9a52f04b5d832dadcded4e554f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4cb5f4c392a4f8b946c2f63dc6a11bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "307cb53eb77348d59e6f7081e03f9a71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import gc\n",
        "model_ref = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Define the model name\n",
        "model_name = model_ref\n",
        "\n",
        "# Load pre-trained model & tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL1BXaUUOFHX",
        "outputId": "2e92b2a3-55d9-4fc1-dadf-f735b20a94d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data size: 148\n",
            "Testing data size: 38\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data size: {len(train_data)}\")\n",
        "print(f\"Testing data size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e231a533a6b14a12929e0225e828713e",
            "38aac622e3424d9685cebefdc4418e65",
            "d34ca7d7de8448abbe46d55428f650fa",
            "66f25f5699134021a2d4e739b94f5a52",
            "fcbf4029c0de4dad819be3d2862b7601",
            "87cddf0aa6864159887f00039bbb9b00",
            "0c7c2ae2b5af416dbe62a610b7ba197d",
            "e530260bf6ff45a1bf9817419c5c1fb4",
            "cf0cb3cf462341bb93f3327a4d40c877",
            "a90584d60cc14b69be02a68639c63447",
            "92252b968b2b4124abdc1a6f18d53e9f",
            "1e1cd510d79946df8d1b513115dd1140",
            "67ca961fb6754e2fb99811fe4a76ca30",
            "28863657ff0b4c97b18dd3aaa6523847",
            "ffbb3c00acfa4023a3fd192f143c5cc2",
            "19b4eebca1fb4d1d8c3a030ab86260fd",
            "a972682ef06b4b59b5cc38a8c57dcdde",
            "40a46b6ccd0c4ba5a81d235b6dbeac26",
            "2254d4f028ac4087acc471ac71324527",
            "397907b456d84994bbd1890457d683d3",
            "13aa8a99b1dc4d649a1ad59ef2caeab0",
            "a6740cbf298141d99c184f40f20c5598"
          ]
        },
        "id": "mVyujLkL-tMg",
        "outputId": "dadf776a-08ee-4b17-b5fc-d407d4fcf015"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e231a533a6b14a12929e0225e828713e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/148 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e1cd510d79946df8d1b513115dd1140",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Data preprocessing, Tokenization, LoRA configuration and training arguments\n",
        "def format_qa(example):\n",
        "    return {\n",
        "        \"text\": f\"Question: {example['input']} Answer: {example['output']}\"\n",
        "    }\n",
        "\n",
        "qa_train_dataset = Dataset.from_list(train_data)\n",
        "formatted_train_dataset = qa_train_dataset.map(format_qa)\n",
        "\n",
        "qa_test_dataset = Dataset.from_list(test_data)\n",
        "formatted_test_dataset = qa_test_dataset.map(format_qa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "9bd1a10bbaad40648265c0403abe6881",
            "973f947502274433838065d7bc2cbd43",
            "086ec3a1ad08466297d3a7a4ab88eb97",
            "54c88bc52a2e4d1f8b03f0a7d87a8171",
            "73518d4d384c4e0ea583fa049c635300",
            "d0a55c2f33f944d78185e45a2e0bcfa0",
            "0e451c05773f4c24ad20b8092743cfd1",
            "479409a399ab486580bb5abaeaf26f0a",
            "c20e47e26e1e41b9b9f4c4f39fd40592",
            "e2d29c1f350242da84f9ef967d649ebb",
            "2fa38b051a2b4686bf1a9b548354f928",
            "c538d6f3ec374acca599b5c01272fbdb",
            "f888893f1ba444a9b28d1d40f2f41137",
            "2b7eb0f69c924cc3924c91770d2f2c96",
            "d5702318381547da92f9c9135e64a4b9",
            "16b705199f8a486a844b8befbf4c6234",
            "3222888a741a49b99d769b471fa708b7",
            "c997a8fd272d4608b80e231d343c20f3",
            "5a0bf04f1e5e4ed08ec30247f096e669",
            "2bc6562006fc4549b979bb6fb799d7fa",
            "2666093450ba4409a9656aef10b95ace",
            "387dfbd5e1c3469daa30f6c5520f2338"
          ]
        },
        "id": "jGxn-d1l-wEC",
        "outputId": "06a03e13-dafe-4f04-afb6-e3b378ae973b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bd1a10bbaad40648265c0403abe6881",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/148 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c538d6f3ec374acca599b5c01272fbdb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tokenization\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Labels should be a shifted version of input_ids for causal LM training\n",
        "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
        "    return inputs\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train_dataset = formatted_train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test_dataset = formatted_test_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CFD7IdIPrVP"
      },
      "source": [
        "# **FINE-TUNE MODEL DENGAN METODE LORA PEFT**\n",
        "\n",
        "Base Model: TinyLlama-1.1B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DlbMo7og-yJt"
      },
      "outputs": [],
      "source": [
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# Wrap model with LoRA\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GjyfTLWU-zu9"
      },
      "outputs": [],
      "source": [
        "# Set Training Arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # Adjusted for GPU memory limitations\n",
        "    gradient_accumulation_steps=8,  # To simulate a larger batch size\n",
        "    warmup_steps=100,\n",
        "    max_steps=250,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    logging_steps=10,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRsVNdFi-9Hr",
        "outputId": "8d916ab5-0abf-4fa4-f793-b386400fe78e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "# Model fine-tune and save\n",
        "\n",
        "# Move model to CPU to free memory before training\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUMe5s11--1h",
        "outputId": "d2f2d83e-fce5-4388-9641-4b5055ee8752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU clache cleared\n"
          ]
        }
      ],
      "source": [
        "# Free up memory before training\n",
        "gc.collect()  # Garbage collection\n",
        "torch.cuda.empty_cache()  # Clears CUDA cache\n",
        "print(\"GPU clache cleared\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C_sopC6K_BL3"
      },
      "outputs": [],
      "source": [
        "# Optimize model with torch.compile (improves execution speed)\n",
        "model = torch.compile(model)\n",
        "\n",
        "# Move model back to GPU for training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "id": "UKTYMyeB-6Xw",
        "outputId": "ce99e91c-1eba-45d5-81fc-85c13d93f565"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 09:13, Epoch 13/14]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>14.781300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>9.759900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.276300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.154800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.069200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.035700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.020800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.018200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.012000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.012200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.011300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.009200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.009700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.008900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.009800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.008700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.008900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.008800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.007900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.008100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.008400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('fine-tuned-QA-tinyllama-1.1B/tokenizer_config.json',\n",
              " 'fine-tuned-QA-tinyllama-1.1B/special_tokens_map.json',\n",
              " 'fine-tuned-QA-tinyllama-1.1B/chat_template.jinja',\n",
              " 'fine-tuned-QA-tinyllama-1.1B/tokenizer.model',\n",
              " 'fine-tuned-QA-tinyllama-1.1B/added_tokens.json',\n",
              " 'fine-tuned-QA-tinyllama-1.1B/tokenizer.json')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine tuned model\n",
        "model.save_pretrained(\"fine-tuned-QA-tinyllama-1.1B\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-QA-tinyllama-1.1B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5qXK_9-5y7C",
        "outputId": "f99e6cd9-26ba-4089-f84e-a3d54f4e8848"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('final-model-merged/tokenizer_config.json',\n",
              " 'final-model-merged/special_tokens_map.json',\n",
              " 'final-model-merged/chat_template.jinja',\n",
              " 'final-model-merged/tokenizer.model',\n",
              " 'final-model-merged/added_tokens.json',\n",
              " 'final-model-merged/tokenizer.json')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Optional: Merge adapter with base model\n",
        "merged_model = model.merge_and_unload()  # This creates a full model: base + adapter\n",
        "\n",
        "# Save the full model\n",
        "merged_model.save_pretrained(\"final-model-merged\")\n",
        "tokenizer.save_pretrained(\"final-model-merged\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y75jYcCqA7WV"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "model_path = \"fine-tuned-QA-tinyllama-1.1B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "def generate_answer(question, max_length=256):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_length=max_length, temperature=0.7, top_k=50, top_p=0.9)\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhxbQD9ABC0D",
        "outputId": "95b38439-9b42-4416-d9f2-893ddf629f53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: How do I exchange a bundle order for a different bundle in the Bajau E-Commerce mobile app?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: Question: How do I exchange a bundle order for a different bundle in the Bajau E-Commerce mobile app?\n",
            "Answer:\n",
            "1. Open the Bajau app\n",
            "2. Tap “My Account”\n",
            "3. Tap “Order History”\n",
            "4. Select the bundle order\n",
            "5. Tap “Exchange”\n",
            "6. Select new bundle\n",
            "7. Contact support with questions\n",
            "\n",
            "Q: What is the process for using a promo code to purchase a subscription in the Bajau E-Commerce mobile app?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: Question: What is the process for using a promo code to purchase a subscription in the Bajau E-Commerce mobile app?\n",
            "Answer:\n",
            "1. Open the Bajau app\n",
            "2. Tap “Subscription”\n",
            "3. Tap “Promo Code”\n",
            "4. Enter promo code\n",
            "5. Apply promo code\n",
            "6. Purchase subscription\n",
            "7. View subscription details\n",
            "\n",
            "Q: How do I access my subscription order history in the Bajau E-Commerce mobile app?\n",
            "A: Question: How do I access my subscription order history in the Bajau E-Commerce mobile app?\n",
            "Answer:\n",
            "1. Open the Bajau app\n",
            "2. Tap “My Account”\n",
            "3. Tap “Subscription Order History”\n",
            "4. View subscription order history\n",
            "5. Select order to view details\n",
            "6. View order status\n",
            "7. Contact support with questions\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example Test Cases - Known Questions\n",
        "test_questions = [\n",
        "    \"How do I exchange a bundle order for a different bundle in the Bajau E-Commerce mobile app?\",\n",
        "    \"What is the process for using a promo code to purchase a subscription in the Bajau E-Commerce mobile app?\",\n",
        "    \"How do I access my subscription order history in the Bajau E-Commerce mobile app?\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {generate_answer(q)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh8VY4TjRJOW",
        "outputId": "cd378656-9410-48fa-91ec-b4915e8ad8e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: How can I swap a bundle order for a different bundle in the Bajau E-Commerce mobile app?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: Question: How can I swap a bundle order for a different bundle in the Bajau E-Commerce mobile app?\n",
            "Answer:\n",
            "1. Open the Bajau app\n",
            "2. Tap “My Account”\n",
            "3. Tap “Order History”\n",
            "4. Select the bundle order\n",
            "5. Tap “Swap for Another Bundle”\n",
            "6. Select the new bundle order\n",
            "7. Contact support with questions\n",
            "\n",
            "Q: What steps do I need to follow to apply a promo code when purchasing a subscription in the Bajau E-Commerce mobile app?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: Question: What steps do I need to follow to apply a promo code when purchasing a subscription in the Bajau E-Commerce mobile app?\n",
            "Answer:\n",
            "1. Open the Bajau app\n",
            "2. Tap “Subscription”\n",
            "3. Tap “Promo Code”\n",
            "4. View promo code details\n",
            "5. Select subscription to apply promo code\n",
            "6. Apply promo code\n",
            "7. View subscription details\n",
            "\n",
            "Q: How can I view my subscription order history in the Bajau E-Commerce mobile app?\n",
            "A: Question: How can I view my subscription order history in the Bajau E-Commerce mobile app?\n",
            "Answer:\n",
            "1. Open the Bajau app\n",
            "2. Tap “Subscription Order History”\n",
            "3. View order history\n",
            "4. Select subscription\n",
            "5. View order details\n",
            "6. View order history\n",
            "7. Contact support with questions\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example Test Cases - Known Questions (Paraphrased)\n",
        "test_questions = [\n",
        "    \"How can I swap a bundle order for a different bundle in the Bajau E-Commerce mobile app?\",\n",
        "    \"What steps do I need to follow to apply a promo code when purchasing a subscription in the Bajau E-Commerce mobile app?\",\n",
        "    \"How can I view my subscription order history in the Bajau E-Commerce mobile app?\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {generate_answer(q)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIPybWQdR0Ex",
        "outputId": "c5f03340-a5ac-41c6-800e-d0b3d2b879f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: How to Log Out from Bajau App?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A: Question: How to Log Out from Bajau App?\n",
            "Answer:\n",
            "1. Open the Bajau app\n",
            "2. Tap “My Account”\n",
            "3. Tap “Log Out”\n",
            "4. Confirm log out\n",
            "5. Exit app\n",
            "6. Add to home screen\n",
            "7. Contact support with questions\n",
            "\n",
            "Q: How to Register to Bajau App?\n",
            "A: Question: How to Register to Bajau App?\n",
            "Answer:\n",
            "1. Open the app\n",
            "2. Tap “My Account”\n",
            "3. Tap “Register”\n",
            "4. Enter name\n",
            "5. Enter email address\n",
            "6. Enter mobile number\n",
            "7. Tap “Create Account”\n",
            "8. Verify phone number\n",
            "9. Set up payment method\n",
            "10. Start using Bajau app\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example Test Cases - Unknown Questions (Tidak ada di dataset)\n",
        "test_questions = [\n",
        "    \"How to Log Out from Bajau App?\",\n",
        "    \"How to Register to Bajau App?\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {generate_answer(q)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SJJbEcBe4ih"
      },
      "source": [
        "# **LOAD DAN EVALUASI METRIK MODEL**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891,
          "referenced_widgets": [
            "afe8b690d6884fdb948bc10517424d36",
            "9fc5b5e7c56a455693224ee634160382",
            "b696c009da1d4cf08361130bf158b44c",
            "94b1874fa6004ef780271d1ca2fd644e",
            "524c6c8d21a9426a98538962d8da2811",
            "c8d3ebd07cec424098af454e087b319f",
            "b36581cb867f49d6ad1232c1f1dfc124",
            "df76c8d3d70949d29a54017654b91d74",
            "206cb0fb22dc492b83a506bae501470b",
            "21e4579c0aab4fb4b2ade85f9a44ace3",
            "e548c6ec951249e0981bd0c75385f015",
            "fb2e8e6f4564444983b1e64a539362d0",
            "46d0e92cec2e425d8c1855e55268184d",
            "f8835f725338468ea1cbb59f370176fa",
            "fb54c273967a4048ba387db7442d8f18",
            "b61bfc9a0d4140f6bb6d953c0c5c2a58",
            "714a44d1e3b54d2fa007bf2be28bf231",
            "b433dcb44737441094df2a1cfb23fd9e",
            "616ce247e50e427dace412d378c31fd2",
            "11c5da0f0b48405abb30dbbc94dad39a",
            "21c2a103d9d246c58e8d0be5b44a672c",
            "d41c102a6c8449e1bd65e809116b8428",
            "09f68b5095584009985cc868c0a1a268",
            "562dc61069144ccea83de162e42055e9",
            "1efbf41ed46d4d65a7c0b2e4a5353d4b",
            "3f15cce02d5e4288bf08536868dd570a",
            "8bdd34d4327643e0940f64e9f1c785e1",
            "2a218404059d47c2b4215b90e2f54149",
            "bbee24b8ae5c478689296559eb568d9c",
            "f380dcf0566a4b2cb7017e6a40086342",
            "603ed292f5d04fb99d6071bad3502e8c",
            "8660f4fd0b8145dbb77d6c27632985b2",
            "62d7e3d8f1654f32920a5ce59b8a34cb",
            "f380402a15d84e94a180a1f2757fed07",
            "8ebd04f0c34d42329203ca4d55d9af9e",
            "a8fd881b972143adac0f2d0172f3ed67",
            "894020219f9c4f47ab2a036817957cc3",
            "1ea1f0c9610e498b9d6464c698d74d18",
            "12d85b869aed44c3b6b2f81a72192ef6",
            "2594f6e460a94301947f50f226655093",
            "a7c18756f04f464c9073c19c8ed0b695",
            "fe9d2ad2241e47d6a616d38015844098",
            "2c2867badba841ebbe6d76da2f5b108e",
            "3c6822165f15447bbd0cc2b48ac5fb4b",
            "a5a8e86841a944eeb57a6f1d645db838",
            "8814cfa1bb814424a9383a8c4e30d875",
            "458a2cb87de74e8a803db7405ce501e8",
            "ab1135750b8245c5983a9fa26ead508b",
            "8df065bf1dcd4625a85dd39b38750361",
            "d07c7ca31fb149c6abee12d7833d9043",
            "827aa3f961c24391a215282c06caf6d4",
            "5333868213b24e08bccd4bd16881d073",
            "10381c3025f64a12879b81fa82ee6c8f",
            "e04140366afb4b8ea7f657b9d0a3d9df",
            "67838546d3f94d549093777999c4d5d2"
          ]
        },
        "id": "hCBPTa_HT6ia",
        "outputId": "eb7396a0-40e6-4b09-96cb-24882cda7af0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afe8b690d6884fdb948bc10517424d36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb2e8e6f4564444983b1e64a539362d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09f68b5095584009985cc868c0a1a268",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f380402a15d84e94a180a1f2757fed07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5a8e86841a944eeb57a6f1d645db838",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/38 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE Scores: {'rouge1': np.float64(0.936710671474426), 'rouge2': np.float64(0.8702513910718245), 'rougeL': np.float64(0.9089164219539468), 'rougeLsum': np.float64(0.9342646553889045)}\n",
            "BLEU Score: {'bleu': 0.8782352519050307, 'precisions': [0.942381562099872, 0.9015748031496063, 0.851278600269179, 0.8225138121546961], 'brevity_penalty': 1.0, 'length_ratio': 1.0006406149903908, 'translation_length': 1562, 'reference_length': 1561}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import evaluate\n",
        "\n",
        "# Load evaluation metrics with the new API\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "bleu_metric  = evaluate.load(\"bleu\")\n",
        "\n",
        "# Function to generate predictions\n",
        "def generate_prediction(question, model, tokenizer, device, max_length=256):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.9\n",
        "        )\n",
        "    # Decode only the generated part\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    # Strip off the prompt\n",
        "    return generated_text.replace(prompt, \"\").strip()\n",
        "\n",
        "# Generate predictions\n",
        "predictions = []\n",
        "references  = []\n",
        "\n",
        "for example in tqdm(test_data):\n",
        "    predictions.append(generate_prediction(\n",
        "        example['input'], model, tokenizer, device\n",
        "    ))\n",
        "    references.append(example['output'])\n",
        "\n",
        "# Compute ROUGE\n",
        "rouge_results = rouge_metric.compute(\n",
        "    predictions=predictions,\n",
        "    references=references,\n",
        "    use_stemmer=True\n",
        ")\n",
        "print(\"ROUGE Scores:\", rouge_results)\n",
        "\n",
        "# Compute BLEU (expects list of reference lists)\n",
        "bleu_references = [[ref] for ref in references]\n",
        "bleu_results    = bleu_metric.compute(\n",
        "    predictions=predictions,\n",
        "    references=bleu_references\n",
        ")\n",
        "print(\"BLEU Score:\", bleu_results)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
